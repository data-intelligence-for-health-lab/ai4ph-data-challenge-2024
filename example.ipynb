{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4PH Data Challenge\n",
    "\n",
    "Summer 2024\n",
    "\n",
    "**Yves Nsoga** \n",
    "PhD Candidate Biomedical Engineering\n",
    "University of Calgary\n",
    "[Data Intelligence for Health Lab](https://cumming.ucalgary.ca/dih)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages here \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions here \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "def stich_image(img_paths: list):\n",
    "    \"\"\"stich images with  overlaping to  generate a panoramic view of the POstal  code\n",
    "\n",
    "    :param img_paths: a list  of image paths\n",
    "    return panoramic_image\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for img_path in img_paths:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        imgs.append(img)\n",
    "    stitchy = cv2.Stitcher.create()\n",
    "    (dummy, stiched_output) = stitchy.stitch(imgs)\n",
    "  \n",
    "    return stiched_output\n",
    "    \n",
    "\n",
    "def crop_and_resize(image: np.array, dimension: tuple = (800, 250)):\n",
    "    \"\"\"crop  and resize image \n",
    "\n",
    "    :param image: Image to resize\n",
    "    :param dimension:  Expected (width, height) of new image, defaults to (800,250)\n",
    "    :Returns image: Resized image\n",
    "    \"\"\"\n",
    "    gray_scaled_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_scaled_img, 0, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(\n",
    "        thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    cnt = cnts[0]\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    img_crop = image[y:y+h, x:x+w]\n",
    "    resized_image = cv2.resize(\n",
    "        img_crop, dimension, interpolation=cv2.INTER_LINEAR)\n",
    "    return resized_image\n",
    "\n",
    "class zone_panorama_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,csv_file,root_dir,transform=None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        :param csv_file: _description_\n",
    "        :type csv_file: _type_\n",
    "        :param root_dir: _description_\n",
    "        :type root_dir: _type_\n",
    "        :param transform: _description_, defaults to None\n",
    "        :type transform: _type_, optional\n",
    "        \"\"\"\n",
    "        self.depravation_indices_data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform =transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.depravation_indices_data)\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index =index.tolist()\n",
    "        image_list = []\n",
    "        image_path = os.path.join(self.root_dir,\n",
    "                                  self.depravation_indices_data.iloc[index, 1])\n",
    "        \n",
    "        try:\n",
    "            for images in os.listdir(image_path):\n",
    "                if (images.endswith(\".png\")):\n",
    "                    image_list.append(os.path.join(image_path,\n",
    "                                                    images))\n",
    "            panorama_view = stich_image(image_list)\n",
    "            image = crop_and_resize(panorama_view)\n",
    "            sample = {\"image\": image,\n",
    "                    \"SCOREMAT\": self.depravation_indices_data.iloc[index, 4], \n",
    "                    \"SCORESOC\": self.depravation_indices_data.iloc[index, 5]}\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "            return sample\n",
    "        except IOError:\n",
    "            pass\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image= sample['image']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        sample['image'] = F.to_tensor(sample['image'])\n",
    "        # image = image.transpose((2, 0, 1))\n",
    "        return sample\n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Convolution  neural network \n",
    "\n",
    "    :param nn: _description_\n",
    "    :type nn: _type_\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "depravation_dataset = zone_panorama_dataset(\n",
    "    csv_file=\"Data challenge dataset_complete_no_duplication.csv\", root_dir=\"GSV_Images_AB\", transform=transforms.Compose([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m----> 2\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mdepravation_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m130\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(depravation_dataset):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample:\n",
      "Cell \u001b[0;32mIn[146], line 77\u001b[0m, in \u001b[0;36mzone_panorama_dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     73\u001b[0m     sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image,\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCOREMAT\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepravation_indices_data\u001b[38;5;241m.\u001b[39miloc[index, \u001b[38;5;241m4\u001b[39m], \n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCORESOC\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepravation_indices_data\u001b[38;5;241m.\u001b[39miloc[index, \u001b[38;5;241m5\u001b[39m]}\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 77\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/data622/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "Cell \u001b[0;32mIn[146], line 92\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     87\u001b[0m image\u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# swap color axis because\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# numpy image: H x W x C\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# torch image: C x H x W\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# image = image.transpose((2, 0, 1))\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "sample = depravation_dataset[130]\n",
    "\n",
    "for i, sample in enumerate(depravation_dataset):\n",
    "    if sample:\n",
    "        plt.figure(figsize=[40, 40])\n",
    "        ax = plt.subplot(1, 6, i + 1)\n",
    "        plt.tight_layout()\n",
    "        ax.set_title(f'Sample #{i} SCOREMAT:{sample[\"SCOREMAT\"]},SCORESOC:{sample[\"SCORESOC\"]}')\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.imshow(sample['image'])\n",
    "\n",
    "    if i == 5:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into  train and test set\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    depravation_dataset, [0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=4,\n",
    "                                       shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_set, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m             image \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/data622/lib/python3.8/site-packages/torch/utils/data/dataset.py:391\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[146], line 77\u001b[0m, in \u001b[0;36mzone_panorama_dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     73\u001b[0m     sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image,\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCOREMAT\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepravation_indices_data\u001b[38;5;241m.\u001b[39miloc[index, \u001b[38;5;241m4\u001b[39m], \n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCORESOC\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepravation_indices_data\u001b[38;5;241m.\u001b[39miloc[index, \u001b[38;5;241m5\u001b[39m]}\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 77\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/data622/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "Cell \u001b[0;32mIn[146], line 92\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     87\u001b[0m image\u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# swap color axis because\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# numpy image: H x W x C\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# torch image: C x H x W\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# image = image.transpose((2, 0, 1))\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_set, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        if data != None:\n",
    "            image = data[\"image\"]\n",
    "            print(image)\n",
    "            mat_score = data[\"SCOREMAT\"]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(image)\n",
    "            loss = criterion(outputs, mat_score)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {\"A\":1,\"B\":2}\n",
    "test1,test2 = test\n",
    "test1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data622",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
